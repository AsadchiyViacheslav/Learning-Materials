{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ce1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgbdz\\AppData\\Local\\Temp\\ipykernel_15724\\4077364254.py:34: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "import os\n",
    "\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "CKPT_PATH = \"models/mobilenetv3_small_best.pth\"\n",
    "ONNX_PATH = \"models/mobilenetv3_small_best.onnx\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "def build_model(num_classes=NUM_CLASSES, pretrained=False):\n",
    "    weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = mobilenet_v3_small(weights=weights)\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = build_model(num_classes=NUM_CLASSES, pretrained=False)\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
    "\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    export_params=True,\n",
    "    opset_version=11, \n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={ \"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"} }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22ccb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12fee7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tf2onnx.convert' has no attribute 'from_onnx_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m onnx_model = onnx.load(onnx_model_path)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Конвертация в TensorFlow SavedModel\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtf2onnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_onnx_model\u001b[49m(\n\u001b[32m     12\u001b[39m     onnx_model,\n\u001b[32m     13\u001b[39m     output_path=saved_model_dir,\n\u001b[32m     14\u001b[39m     output_format=\u001b[33m\"\u001b[39m\u001b[33msaved_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     opset=\u001b[32m11\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tf2onnx.convert' has no attribute 'from_onnx_model'"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "onnx_model_path = \"model.onnx\"\n",
    "saved_model_dir = \"mobilenetv3_saved_model\"\n",
    "\n",
    "# Загружаем ONNX модель\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Конвертация в TensorFlow SavedModel\n",
    "tf2onnx.convert.from_onnx_model(\n",
    "    onnx_model,\n",
    "    output_path=saved_model_dir,\n",
    "    output_format=\"saved_model\",\n",
    "    opset=11\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
