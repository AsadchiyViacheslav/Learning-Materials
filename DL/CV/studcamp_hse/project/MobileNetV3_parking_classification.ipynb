{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bcde84-80d1-49b1-9336-f61df4a169ee",
   "metadata": {},
   "source": [
    "## Импорты, параметры и константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8ddc02-2d81-4d19-a4ab-06825919072f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:10:39.501025Z",
     "iopub.status.busy": "2025-10-16T17:10:39.500007Z",
     "iopub.status.idle": "2025-10-16T17:10:52.657170Z",
     "shell.execute_reply": "2025-10-16T17:10:52.656449Z",
     "shell.execute_reply.started": "2025-10-16T17:10:39.500979Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "DEFAULT_TRAIN_CSV = \"data/parking_label_train.csv\"\n",
    "DEFAULT_TEST_CSV  = \"data/parking_label_test.csv\"\n",
    "OUTPUT_DIR = \"MobileNetV3\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = min(16, os.cpu_count())\n",
    "NUM_EPOCHS = 100\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "PATIENCE = 30\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "PRETRAINED = True\n",
    "DATA_FRACTION = 1 \n",
    "MIN_FREE_GB = 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46367277-20ca-4fc3-a87e-a0ede1e92d3c",
   "metadata": {},
   "source": [
    "## Архитектура (модель, агументация, загрузка данных и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59833a5-94b0-447b-a919-6c104b5ea083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:12:15.248921Z",
     "iopub.status.busy": "2025-10-16T17:12:15.247701Z",
     "iopub.status.idle": "2025-10-16T17:12:15.295824Z",
     "shell.execute_reply": "2025-10-16T17:12:15.295083Z",
     "shell.execute_reply.started": "2025-10-16T17:12:15.248874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class RandomRotate90:\n",
    "    \"\"\"Случайный поворот на 0, 90, 180 или 270 градусов\"\"\"\n",
    "    def __call__(self, img):\n",
    "        angle = np.random.choice([0, 90, 180, 270])\n",
    "        return img.rotate(angle) if angle != 0 else img\n",
    "\n",
    "\n",
    "class ScooterDataset(Dataset):\n",
    "    def __init__(self, csv_file, class_to_idx=None, transforms=None, sample_frac=1.0, cache=False, min_free_gb=1.0, max_cache_images = 50000):\n",
    "        if isinstance(csv_file, pd.DataFrame):\n",
    "            self.df = csv_file.copy()\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "\n",
    "        if 0 < sample_frac < 1.0:\n",
    "            self.df = self.df.sample(frac=sample_frac, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "        self.labels = self.df[\"parking\"].astype(str).tolist()\n",
    "        self.paths = self.df[\"path\"].tolist()\n",
    "\n",
    "        if class_to_idx is None:\n",
    "            classes = sorted(list(set(self.labels)))\n",
    "            self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "\n",
    "        self.targets = [self.class_to_idx[l] for l in self.labels]\n",
    "        self.transforms = transforms\n",
    "        self.cache = cache\n",
    "        self.min_free_gb = min_free_gb\n",
    "        self.cached_images = {}\n",
    "        self.max_cache_images = max_cache_images\n",
    "\n",
    "        if self.cache:\n",
    "            self._cache_images_safely()\n",
    "\n",
    "    def _cache_images_safely(self):\n",
    "        total_cached = 0\n",
    "        for idx, p in enumerate(tqdm(self.paths, mininterval=3, desc=\"Caching images\")):\n",
    "            if total_cached >= self.max_cache_images:\n",
    "                print(f\"Reached max_cache_images limit: {self.max_cache_images}\")\n",
    "                break\n",
    "                \n",
    "            mem = psutil.virtual_memory()\n",
    "            free_gb = mem.available / 1e9\n",
    "            if free_gb < self.min_free_gb:\n",
    "                print(f\"Stopping caching: only {free_gb:.2f} GB free after {total_cached} images\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "            except:\n",
    "                img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "\n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "            self.cached_images[idx] = img\n",
    "            total_cached += 1\n",
    "\n",
    "        print(f\"Cached {total_cached} / {len(self.paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.targets[idx]\n",
    "        if idx in self.cached_images:\n",
    "            img = self.cached_images[idx]\n",
    "        else:\n",
    "            p = self.paths[idx]\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "            except:\n",
    "                img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def get_transforms(img_size=IMG_SIZE):\n",
    "    train_transforms = transforms.Compose([\n",
    "        RandomRotate90(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3)], p=0.7),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "\n",
    "def build_model(num_classes=NUM_CLASSES, pretrained=True):\n",
    "    weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = mobilenet_v3_small(weights=weights)\n",
    "    if hasattr(model, 'classifier'):\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, log=True):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    running_loss = 0.0\n",
    "    inference_times = []\n",
    "\n",
    "    iterator = tqdm(dataloader, desc=\"Evaluating\", leave=False) if log else dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in iterator:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(imgs)\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    avg_time_per_image = np.mean(inference_times) / imgs.size(0)\n",
    "    total_time_per_image = np.sum(inference_times) / len(dataloader.dataset)\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Average inference time per image: {total_time_per_image*1000:.2f} ms\")\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1,\n",
    "        'confusion_matrix': cm, 'avg_inference_time': total_time_per_image\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_test(model, dataloader, criterion, device, class_names, log=True):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    running_loss = 0.0\n",
    "    inference_times = []\n",
    "\n",
    "    iterator = tqdm(dataloader, desc=\"Evaluating on TEST set\", leave=False) if log else dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in iterator:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(imgs)\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    print(\"\\n---------------- Final TEST Evaluation ----------------\")\n",
    "    print(f\"Loss:      {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds, \n",
    "        target_names=class_names, \n",
    "        output_dict=True, \n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(df_report.round(4))\n",
    "\n",
    "    avg_time_per_image = np.sum(inference_times) / len(dataloader.dataset)\n",
    "    print(f\"\\nAverage inference time per image: {avg_time_per_image*1000:.2f} ms\")\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class': df_report,\n",
    "        'avg_inference_time': avg_time_per_image\n",
    "    }\n",
    "\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, criterion, optimizer, device, num_epochs=NUM_EPOCHS):\n",
    "    best_val_f1 = -float('inf')\n",
    "    best_epoch = -1\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_cpu_time = 0.0\n",
    "        total_gpu_time = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for imgs, labels in pbar:\n",
    "            start_cpu = time.time()\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            total_cpu_time += time.time() - start_cpu\n",
    "\n",
    "            start_gpu = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_gpu_time += time.time() - start_gpu\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            processed = min(((pbar.n + 1) * imgs.size(0)), len(train_loader.dataset))\n",
    "            avg_loss_running = running_loss / processed\n",
    "            pbar.set_postfix({'loss': f\"{avg_loss_running:.4f}\"})\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Валидация\n",
    "        start_val = time.time()\n",
    "        val_res = evaluate(model, val_loader, criterion, device, False)\n",
    "        val_time = time.time() - start_val\n",
    "\n",
    "        total_time = total_cpu_time + total_gpu_time + val_time\n",
    "        cpu_percent = total_cpu_time / total_time * 100\n",
    "        gpu_percent = total_gpu_time / total_time * 100\n",
    "        val_percent = val_time / total_time * 100\n",
    "\n",
    "        print(f\"Epoch {epoch} summary: total_time={total_time:.1f}s, CPU={cpu_percent:.1f}%, GPU={gpu_percent:.1f}%, VAL={val_percent:.1f}%\")\n",
    "        print(f\"Train loss={epoch_train_loss:.4f}, Val loss={val_res['loss']:.4f}, Val f1={val_res['f1']:.4f}, Val acc={val_res['accuracy']:.4f}\")\n",
    "\n",
    "        if val_res['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_res['f1']\n",
    "            best_epoch = epoch\n",
    "            ckpt_path = os.path.join(OUTPUT_DIR, 'mobilenetv3_parking.pth')\n",
    "            torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict()}, ckpt_path)\n",
    "            print(f\"Saved best model to {ckpt_path}\")\n",
    "\n",
    "        if epoch - best_epoch >= PATIENCE:\n",
    "            print(f\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(f\"Training finished. Best epoch: {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af6b6e-95d8-44b8-b62e-af73e1966700",
   "metadata": {},
   "source": [
    "## Загрузка данных и кэширование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a75eac-847a-4bd0-a176-8dfdce7734ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:13:20.064146Z",
     "iopub.status.busy": "2025-10-16T17:13:20.062997Z",
     "iopub.status.idle": "2025-10-16T17:13:20.215076Z",
     "shell.execute_reply": "2025-10-16T17:13:20.214353Z",
     "shell.execute_reply.started": "2025-10-16T17:13:20.064110Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA L4\n",
      "Class to idx: {'hard_to_say': 0, 'inside': 1, 'outside': 2}\n",
      "Train samples: 32319, Val samples: 3592, Test samples: 8978\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "train_transforms, val_transforms = get_transforms(IMG_SIZE)\n",
    "\n",
    "full_train_df = pd.read_csv(DEFAULT_TRAIN_CSV)\n",
    "train_df, val_df = train_test_split(\n",
    "    full_train_df, \n",
    "    test_size=0.1, \n",
    "    random_state=SEED, \n",
    "    stratify=full_train_df['parking']\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(DEFAULT_TEST_CSV)\n",
    "\n",
    "if 0 < DATA_FRACTION < 1.0:\n",
    "    train_df = train_df.sample(frac=DATA_FRACTION, random_state=SEED).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=DATA_FRACTION, random_state=SEED).reset_index(drop=True)\n",
    "    test_df  = test_df.sample(frac=DATA_FRACTION, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "classes = sorted(train_df['parking'].unique())\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "print(\"Class to idx:\", class_to_idx)\n",
    "\n",
    "train_dataset = ScooterDataset(train_df, class_to_idx=class_to_idx, transforms=train_transforms, cache=False, min_free_gb=MIN_FREE_GB)\n",
    "val_dataset   = ScooterDataset(val_df,   class_to_idx=class_to_idx, transforms=val_transforms,   cache=False, min_free_gb=MIN_FREE_GB)\n",
    "test_dataset  = ScooterDataset(test_df, class_to_idx=class_to_idx, transforms=val_transforms, cache=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "counts = Counter(train_dataset.targets)\n",
    "class_counts = [counts[i] for i in range(len(class_to_idx))]\n",
    "total = sum(class_counts)\n",
    "class_weights = [total/(len(class_counts)*c) if c>0 else 0.0 for c in class_counts]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "pin_memory = DEVICE == \"cuda\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=pin_memory)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_memory)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a5323-8a96-4888-9500-6fae0579497f",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abfb174-053f-4755-b3b2-9671adfd4de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T15:25:32.965217Z",
     "iopub.status.busy": "2025-10-16T15:25:32.964536Z",
     "iopub.status.idle": "2025-10-16T15:36:43.762070Z",
     "shell.execute_reply": "2025-10-16T15:36:43.761165Z",
     "shell.execute_reply.started": "2025-10-16T15:25:32.965192Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc1743bf9004c078b93b5e7cc6370b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 1 summary: total_time=8.7s, CPU=1.1%, GPU=50.1%, VAL=48.8%\n",
      "Train loss=0.8084, Val loss=0.7441, Val f1=0.6224, Val acc=0.6868\n",
      "Saved best model to MobileNetV3/mobilenetv3_parking.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073d47eecbed428ab3470ab7ebe289c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 2 summary: total_time=8.6s, CPU=0.9%, GPU=49.3%, VAL=49.8%\n",
      "Train loss=0.7030, Val loss=0.7680, Val f1=0.6291, Val acc=0.7258\n",
      "Saved best model to MobileNetV3/mobilenetv3_parking.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3069330af02b48c7af070b9d69c98c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 3 summary: total_time=8.5s, CPU=1.0%, GPU=49.2%, VAL=49.7%\n",
      "Train loss=0.6287, Val loss=0.7973, Val f1=0.5915, Val acc=0.6556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbfbc2f5004450899137048569b1ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 4 summary: total_time=8.9s, CPU=1.0%, GPU=50.2%, VAL=48.9%\n",
      "Train loss=0.5303, Val loss=0.8347, Val f1=0.5988, Val acc=0.6826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd0330a94f14e1fab30514036908510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 5 summary: total_time=8.6s, CPU=1.0%, GPU=49.3%, VAL=49.7%\n",
      "Train loss=0.4124, Val loss=1.2145, Val f1=0.5981, Val acc=0.7291\n",
      "Saved best model to MobileNetV3/mobilenetv3_parking.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68df292cbaad4d24b5ad4fbeccf6fca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 6 summary: total_time=8.7s, CPU=0.9%, GPU=50.4%, VAL=48.7%\n",
      "Train loss=0.2962, Val loss=1.1866, Val f1=0.6174, Val acc=0.7149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5f964608a24a678e8a7b5046d5a815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 7 summary: total_time=8.6s, CPU=1.1%, GPU=49.3%, VAL=49.7%\n",
      "Train loss=0.2098, Val loss=1.7492, Val f1=0.5991, Val acc=0.7080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0fbc2fa05c44eb9c97f8c2e58eabfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 8 summary: total_time=8.6s, CPU=0.9%, GPU=48.8%, VAL=50.3%\n",
      "Train loss=0.1549, Val loss=1.6124, Val f1=0.5799, Val acc=0.6712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8484d0bb8ee041a696fd2b76735e78fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 9 summary: total_time=8.6s, CPU=0.8%, GPU=50.3%, VAL=49.0%\n",
      "Train loss=0.1387, Val loss=1.9904, Val f1=0.6227, Val acc=0.7355\n",
      "Saved best model to MobileNetV3/mobilenetv3_parking.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4227887c84dbe988ced9e250ddb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 10 summary: total_time=8.8s, CPU=1.0%, GPU=50.9%, VAL=48.2%\n",
      "Train loss=0.1017, Val loss=2.0291, Val f1=0.6251, Val acc=0.7252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91138570dc3547688269da479cadddae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 11 summary: total_time=8.9s, CPU=0.8%, GPU=49.3%, VAL=49.9%\n",
      "Train loss=0.0868, Val loss=2.2068, Val f1=0.5956, Val acc=0.7055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbc144e5c5b435ba06a00870cd0f4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 12 summary: total_time=8.8s, CPU=0.9%, GPU=50.1%, VAL=49.0%\n",
      "Train loss=0.0745, Val loss=2.4910, Val f1=0.5987, Val acc=0.7138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1600c3ab203d4f0ca38770dc9dd9f529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 13 summary: total_time=8.8s, CPU=0.9%, GPU=49.8%, VAL=49.3%\n",
      "Train loss=0.0705, Val loss=2.2419, Val f1=0.6131, Val acc=0.7194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae797dc681f24799a1667eed8e5286f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 14 summary: total_time=8.7s, CPU=0.8%, GPU=50.4%, VAL=48.7%\n",
      "Train loss=0.0798, Val loss=2.0420, Val f1=0.6056, Val acc=0.7066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d1661f3ded4ce19b1a0c61affe1630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 15 summary: total_time=8.8s, CPU=1.0%, GPU=50.1%, VAL=48.9%\n",
      "Train loss=0.0608, Val loss=2.3996, Val f1=0.6118, Val acc=0.7066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6c05dc0996449b872d0fa74acd1c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 16 summary: total_time=8.8s, CPU=0.8%, GPU=50.9%, VAL=48.3%\n",
      "Train loss=0.0533, Val loss=2.6744, Val f1=0.5852, Val acc=0.7152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc5244d3428445d98c74959ff4fbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 17 summary: total_time=8.7s, CPU=1.0%, GPU=49.4%, VAL=49.6%\n",
      "Train loss=0.0562, Val loss=2.9667, Val f1=0.5954, Val acc=0.7263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a63ddbfd7df492bb34998ab39e72a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 18 summary: total_time=9.0s, CPU=0.9%, GPU=50.6%, VAL=48.5%\n",
      "Train loss=0.0590, Val loss=2.3783, Val f1=0.6053, Val acc=0.7144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d790d0dfe8f44fa8ec9d10df89b35ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 19 summary: total_time=8.7s, CPU=1.0%, GPU=49.7%, VAL=49.3%\n",
      "Train loss=0.0494, Val loss=2.6261, Val f1=0.6079, Val acc=0.7252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c98c2aff88425a8626243bec18fe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 20 summary: total_time=8.7s, CPU=0.9%, GPU=49.2%, VAL=49.9%\n",
      "Train loss=0.0405, Val loss=3.4078, Val f1=0.5919, Val acc=0.7333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa145f5866a47b39a0375f4fcb04b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 21 summary: total_time=8.6s, CPU=0.9%, GPU=49.8%, VAL=49.3%\n",
      "Train loss=0.0640, Val loss=2.4791, Val f1=0.6068, Val acc=0.7280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eb1446165f4929b615706f51af7f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 22 summary: total_time=8.7s, CPU=1.0%, GPU=49.3%, VAL=49.7%\n",
      "Train loss=0.0404, Val loss=2.8166, Val f1=0.6019, Val acc=0.7261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32eb8ce41a344afb238bceed9ecd09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 23 summary: total_time=8.6s, CPU=0.9%, GPU=49.9%, VAL=49.2%\n",
      "Train loss=0.0390, Val loss=2.8089, Val f1=0.5964, Val acc=0.7146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795d4a960f4c46eeb72a89d1451ee958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 24 summary: total_time=8.9s, CPU=0.8%, GPU=49.4%, VAL=49.9%\n",
      "Train loss=0.0388, Val loss=2.7128, Val f1=0.5888, Val acc=0.7272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3057c36a09a145c19dee91a496c0b63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 25 summary: total_time=9.0s, CPU=0.9%, GPU=50.6%, VAL=48.5%\n",
      "Train loss=0.0447, Val loss=2.7106, Val f1=0.6020, Val acc=0.7216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc1036ea81e48d1b9c8c2d67d1110e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 26 summary: total_time=8.8s, CPU=1.1%, GPU=49.3%, VAL=49.6%\n",
      "Train loss=0.0462, Val loss=2.4028, Val f1=0.6002, Val acc=0.6979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb9c750d04b489c818ed5f33a5cc74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 27 summary: total_time=8.9s, CPU=0.9%, GPU=51.7%, VAL=47.3%\n",
      "Train loss=0.0380, Val loss=2.7533, Val f1=0.6036, Val acc=0.7096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16be62c293d422582e701665235035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 28 summary: total_time=8.8s, CPU=0.9%, GPU=48.9%, VAL=50.3%\n",
      "Train loss=0.0406, Val loss=2.6226, Val f1=0.5984, Val acc=0.7166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c852ceba3aa14dce99e190793a03c5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 29 summary: total_time=8.9s, CPU=0.8%, GPU=51.1%, VAL=48.2%\n",
      "Train loss=0.0329, Val loss=2.9203, Val f1=0.6029, Val acc=0.7208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069572eb900f4356948834ed179dae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 30 summary: total_time=8.7s, CPU=0.8%, GPU=49.7%, VAL=49.6%\n",
      "Train loss=0.0385, Val loss=2.6997, Val f1=0.5800, Val acc=0.6901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411064ea034942ba910a1a115a2bb3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 31 summary: total_time=8.7s, CPU=1.2%, GPU=49.0%, VAL=49.8%\n",
      "Train loss=0.0435, Val loss=2.7320, Val f1=0.5885, Val acc=0.7088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561868ae25254ece888b054b880439c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 32 summary: total_time=8.9s, CPU=0.8%, GPU=50.3%, VAL=48.8%\n",
      "Train loss=0.0292, Val loss=2.8964, Val f1=0.6044, Val acc=0.7216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d342e5912b2845ad973df4d9612d53b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 33 summary: total_time=8.7s, CPU=0.9%, GPU=49.5%, VAL=49.6%\n",
      "Train loss=0.0309, Val loss=2.4376, Val f1=0.6075, Val acc=0.7038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1845e3cb152746b1956727957c5bb4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 34 summary: total_time=8.8s, CPU=0.8%, GPU=50.0%, VAL=49.2%\n",
      "Train loss=0.0311, Val loss=2.9212, Val f1=0.5910, Val acc=0.7057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d279ede192a44438c426f5efd851fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 35 summary: total_time=8.6s, CPU=1.0%, GPU=48.6%, VAL=50.4%\n",
      "Train loss=0.0453, Val loss=2.6613, Val f1=0.5930, Val acc=0.7085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0480bd39939e4450ac734892624258e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 36 summary: total_time=8.6s, CPU=1.1%, GPU=49.3%, VAL=49.6%\n",
      "Train loss=0.0327, Val loss=2.7586, Val f1=0.6071, Val acc=0.7224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d45faf1e1c4bce8bf52d70584e7153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 37 summary: total_time=9.0s, CPU=0.8%, GPU=51.8%, VAL=47.4%\n",
      "Train loss=0.0295, Val loss=2.9416, Val f1=0.6056, Val acc=0.7261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd0974243643bda99b5c603fcb3db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 38 summary: total_time=9.1s, CPU=0.8%, GPU=51.5%, VAL=47.7%\n",
      "Train loss=0.0315, Val loss=2.9079, Val f1=0.6035, Val acc=0.7102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f3221e41fe45468969794a25541779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39/100:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per image: 0.04 ms\n",
      "Epoch 39 summary: total_time=8.9s, CPU=0.8%, GPU=50.3%, VAL=48.9%\n",
      "Train loss=0.0336, Val loss=2.8077, Val f1=0.5881, Val acc=0.7002\n",
      "Early stopping triggered\n",
      "Training finished. Best epoch: 9\n"
     ]
    }
   ],
   "source": [
    "model = build_model(num_classes=len(class_to_idx), pretrained=PRETRAINED).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_loop(train_loader, val_loader, model, criterion, optimizer, DEVICE, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3896b-7cd1-44a3-aac3-e5f30233d8c2",
   "metadata": {},
   "source": [
    "## Оценка на test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8909ae03-7a1a-4e5b-9e86-04f9ef64ad16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:13:54.586725Z",
     "iopub.status.busy": "2025-10-16T17:13:54.585664Z",
     "iopub.status.idle": "2025-10-16T17:16:54.000907Z",
     "shell.execute_reply": "2025-10-16T17:16:53.999989Z",
     "shell.execute_reply.started": "2025-10-16T17:13:54.586686Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Final evaluation on TEST set ----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on TEST set:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Final TEST Evaluation ----------------\n",
      "Loss:      2.0138\n",
      "Accuracy:  0.7215\n",
      "Precision: 0.6117\n",
      "Recall:    0.6077\n",
      "F1-score:  0.6088\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1235  463  427]\n",
      " [ 533 4724  239]\n",
      " [ 530  308  519]]\n",
      "\n",
      "Per-class metrics:\n",
      "              precision  recall  f1-score    support\n",
      "hard_to_say      0.5374  0.5812    0.5584  2125.0000\n",
      "inside           0.8597  0.8595    0.8596  5496.0000\n",
      "outside          0.4380  0.3825    0.4083  1357.0000\n",
      "accuracy         0.7215  0.7215    0.7215     0.7215\n",
      "macro avg        0.6117  0.6077    0.6088  8978.0000\n",
      "weighted avg     0.7197  0.7215    0.7201  8978.0000\n",
      "\n",
      "Average inference time per image: 0.67 ms\n"
     ]
    }
   ],
   "source": [
    "model = build_model(num_classes=len(class_to_idx), pretrained=PRETRAINED).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "best_ckpt = os.path.join(OUTPUT_DIR, 'mobilenetv3_parking.pth')\n",
    "\n",
    "if os.path.exists(best_ckpt):\n",
    "    checkpoint = torch.load(best_ckpt, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "    print(\"\\n---------------- Final evaluation on TEST set ----------------\")\n",
    "    test_results = evaluate_test(model, test_loader, criterion, DEVICE, class_names=list(class_to_idx.keys()))\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
